[{"id":"d375ecc4-dfd6-4eac-815e-6f505f1f1e5e","userId":"d9330366-68f4-451f-a001-10bf00b11fd3","function":{"id":"gemini_ai_studio_","name":"Gemini ai studio ","meta":{"description":"Google AI Studio (Gemini) Integration for OpenWebUI","type":"pipe","manifest":{"title":"Google AI Studio (Gemini) Integration for OpenWebUI","environment_variables":""}},"content":"\"\"\"\ntitle: Google AI Studio (Gemini) Integration for OpenWebUI\nenvironment_variables:\n    - GEMINI_API_KEY (required)\n\"\"\"\nimport json\nimport logging\nfrom typing import List, Dict, Union, AsyncIterator\nfrom pydantic import BaseModel\nfrom open_webui.utils.misc import pop_system_message\nimport aiohttp\n\n\nclass Pipe:\n    # Using the native Gemini API endpoint\n    API_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/models\"\n\n    class Valves(BaseModel):\n        GEMINI_API_KEY: str = \"Your API Key Here\"\n        DEBUG: bool = True\n        FORCE_NON_STREAMING: bool = (\n            False  # Set to True to disable streaming temporarily\n        )\n\n    def __init__(self):\n        logging.basicConfig(level=logging.INFO)\n        self.type = \"manifold\"\n        self.id = \"gemini\"\n        self.name = \"Gemini\"\n        self.valves = self.Valves()\n\n    def get_gemini_models(self) -> List[dict]:\n        \"\"\"Return all supported Gemini models with updated names\"\"\"\n        models = [\n            {\n                \"id\": \"gemini-2.5-pro\",\n                \"name\": \"Gemini 2.5 pro\",\n                \"context_length\": 1048576,\n                \"supports_vision\": True,\n                \"description\": \"Best price-performance model with well-rounded capabilities and thinking support\",\n            },\n            {\n                \"id\": \"gemini-2.5-flash\",\n                \"name\": \"Gemini 2.5 Flash\",\n                \"context_length\": 1048576,\n                \"supports_vision\": True,\n                \"description\": \"Most powerful thinking model with maximum response accuracy\",\n            },\n            {\n                \"id\": \"gemini-2.5-flash-lite\",\n                \"name\": \"Gemini 2.5 Flash-Lite\",\n                \"context_length\": 1048576,\n                \"supports_vision\": True,\n                \"description\": \"Most powerful thinking model with maximum response accuracy\",\n            },\n            {\n                \"id\": \"gemini-2.0-flash\",\n                \"name\": \"Gemini 2.0 Flash\",\n                \"context_length\": 1048576,\n                \"supports_vision\": True,\n                \"description\": \"Newest multimodal model with next generation features and improved capabilities\",\n            },\n            {\n                \"id\": \"gemini-2.0-flash-lite\",\n                \"name\": \"Gemini 2.0 Flash Lite\",\n                \"context_length\": 1048576,\n                \"supports_vision\": True,\n                \"description\": \"Optimized for cost efficiency and low latency\",\n            },\n            {\n                \"id\": \"gemini-1.5-flash\",\n                \"name\": \"Gemini 1.5 Flash\",\n                \"context_length\": 1048576,\n                \"supports_vision\": True,\n                \"description\": \"Fast and versatile performance for diverse tasks\",\n            },\n            {\n                \"id\": \"gemini-1.5-flash-8b\",\n                \"name\": \"Gemini 1.5 Flash 8B\",\n                \"context_length\": 1048576,\n                \"supports_vision\": True,\n                \"description\": \"8B parameter model for high volume tasks\",\n            },\n        ]\n        return models\n\n    def pipes(self) -> List[dict]:\n        return self.get_gemini_models()\n\n    async def pipe(\n        self, body: Dict, __event_emitter__=None\n    ) -> Union[str, AsyncIterator[str]]:\n        \"\"\"Process a request to the Gemini API using the native endpoint.\"\"\"\n\n        if (\n            not self.valves.GEMINI_API_KEY\n            or self.valves.GEMINI_API_KEY == \"Your API Key Here\"\n        ):\n            error_msg = \"Error: GEMINI_API_KEY is required. Please configure it in the function settings.\"\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\"type\": \"status\", \"data\": {\"description\": error_msg, \"done\": True}}\n                )\n            return error_msg\n\n        try:\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\"description\": \"Processing request...\", \"done\": False},\n                    }\n                )\n\n            # Extract system message and regular messages\n            system_message, messages = pop_system_message(body[\"messages\"])\n            model_name = body[\"model\"]\n\n            # Convert messages to Gemini format\n            formatted_contents = []\n\n            # Add system message as first user message if present\n            if system_message:\n                formatted_contents.append(\n                    {\n                        \"role\": \"user\",\n                        \"parts\": [{\"text\": f\"[SYSTEM INSTRUCTION]: {system_message}\"}],\n                    }\n                )\n                formatted_contents.append(\n                    {\n                        \"role\": \"model\",\n                        \"parts\": [\n                            {\"text\": \"I understand. I will follow these instructions.\"}\n                        ],\n                    }\n                )\n\n            # Process messages\n            for message in messages:\n                role = \"user\" if message[\"role\"] == \"user\" else \"model\"\n\n                if isinstance(message[\"content\"], str):\n                    # Simple text message\n                    formatted_contents.append(\n                        {\"role\": role, \"parts\": [{\"text\": message[\"content\"]}]}\n                    )\n                else:\n                    # Handle multimodal content\n                    parts = []\n\n                    for item in message[\"content\"]:\n                        if item[\"type\"] == \"text\":\n                            parts.append({\"text\": item[\"text\"]})\n                        elif item[\"type\"] == \"image_url\" and \"url\" in item[\"image_url\"]:\n                            if item[\"image_url\"][\"url\"].startswith(\"data:image\"):\n                                # Process base64 image\n                                mime_type = (\n                                    item[\"image_url\"][\"url\"].split(\";\")[0].split(\":\")[1]\n                                )\n                                base64_data = item[\"image_url\"][\"url\"].split(\",\")[1]\n                                parts.append(\n                                    {\n                                        \"inline_data\": {\n                                            \"mime_type\": mime_type,\n                                            \"data\": base64_data,\n                                        }\n                                    }\n                                )\n                            else:\n                                # For URL images, we'd need to fetch them first\n                                parts.append(\n                                    {\"text\": f\"[Image URL: {item['image_url']['url']}]\"}\n                                )\n\n                    formatted_contents.append({\"role\": role, \"parts\": parts})\n\n            # Create the payload for Gemini API\n            payload = {\n                \"contents\": formatted_contents,\n                \"generationConfig\": {\n                    \"temperature\": body.get(\"temperature\", 0.7),\n                    \"topP\": body.get(\"top_p\", 0.95),\n                    \"maxOutputTokens\": body.get(\"max_tokens\", 2048),\n                },\n            }\n\n            # Build the URL - Remove any prefix that might be added by the manifold\n            # The model name should be used as-is from the models list\n            clean_model_name = model_name.replace(\"gemini_api.\", \"\").replace(\n                \"gemini.\", \"\"\n            )\n            url = f\"{self.API_BASE_URL}/{clean_model_name}:generateContent\"\n\n            # Add API key as query parameter\n            url += f\"?key={self.valves.GEMINI_API_KEY}\"\n\n            # Headers\n            headers = {\n                \"Content-Type\": \"application/json\",\n            }\n\n            if self.valves.DEBUG:\n                print(f\"[GEMINI] Original model: {model_name}\")\n                print(f\"[GEMINI] Clean model: {clean_model_name}\")\n                print(f\"[GEMINI] URL: {url}\")\n                print(f\"[GEMINI] Payload: {json.dumps(payload, indent=2)}\")\n\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": \"Sending request to Gemini API...\",\n                            \"done\": False,\n                        },\n                    }\n                )\n\n            # Handle streaming vs non-streaming\n            use_streaming = (\n                body.get(\"stream\", False) and not self.valves.FORCE_NON_STREAMING\n            )\n\n            if self.valves.DEBUG:\n                print(f\"[GEMINI] Stream requested: {body.get('stream', False)}\")\n                print(\n                    f\"[GEMINI] Force non-streaming: {self.valves.FORCE_NON_STREAMING}\"\n                )\n                print(f\"[GEMINI] Using streaming: {use_streaming}\")\n\n            if use_streaming:\n                return self._handle_streaming(url, headers, payload, __event_emitter__)\n            else:\n                return await self._handle_normal(\n                    url, headers, payload, __event_emitter__\n                )\n\n        except Exception as e:\n            error_msg = f\"Error: {str(e)}\"\n            if self.valves.DEBUG:\n                print(f\"[GEMINI] Exception: {error_msg}\")\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\"type\": \"status\", \"data\": {\"description\": error_msg, \"done\": True}}\n                )\n            return error_msg\n\n    async def _handle_normal(self, url, headers, payload, __event_emitter__):\n        \"\"\"Handle non-streaming request\"\"\"\n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    url, headers=headers, json=payload, timeout=60\n                ) as response:\n                    response_text = await response.text()\n\n                    if self.valves.DEBUG:\n                        print(f\"[GEMINI] Response status: {response.status}\")\n                        print(f\"[GEMINI] Response: {response_text}\")\n\n                    if response.status != 200:\n                        error_msg = f\"Error: HTTP {response.status}: {response_text}\"\n                        if __event_emitter__:\n                            await __event_emitter__(\n                                {\n                                    \"type\": \"status\",\n                                    \"data\": {\"description\": error_msg, \"done\": True},\n                                }\n                            )\n                        return error_msg\n\n                    data = json.loads(response_text)\n\n                    # Extract response text from Gemini format\n                    if \"candidates\" in data and len(data[\"candidates\"]) > 0:\n                        candidate = data[\"candidates\"][0]\n                        if \"content\" in candidate and \"parts\" in candidate[\"content\"]:\n                            parts = candidate[\"content\"][\"parts\"]\n                            if len(parts) > 0 and \"text\" in parts[0]:\n                                response_text = parts[0][\"text\"]\n                            else:\n                                response_text = \"No text response generated\"\n                        else:\n                            response_text = \"No content in response\"\n                    else:\n                        response_text = \"No candidates in response\"\n\n                    if __event_emitter__:\n                        await __event_emitter__(\n                            {\n                                \"type\": \"status\",\n                                \"data\": {\n                                    \"description\": \"Request completed\",\n                                    \"done\": True,\n                                },\n                            }\n                        )\n\n                    return response_text\n        except Exception as e:\n            error_msg = f\"Request error: {str(e)}\"\n            if self.valves.DEBUG:\n                print(f\"[GEMINI] Request exception: {error_msg}\")\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\"type\": \"status\", \"data\": {\"description\": error_msg, \"done\": True}}\n                )\n            return error_msg\n\n    async def _handle_streaming(self, url, headers, payload, __event_emitter__):\n        \"\"\"Handle streaming request\"\"\"\n        try:\n            # Modify URL for streaming\n            stream_url = url.replace(\":generateContent\", \":streamGenerateContent\")\n\n            if self.valves.DEBUG:\n                print(f\"[GEMINI] Streaming URL: {stream_url}\")\n\n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    stream_url, headers=headers, json=payload, timeout=60\n                ) as response:\n                    if response.status != 200:\n                        error_text = await response.text()\n                        error_msg = f\"Error: HTTP {response.status}: {error_text}\"\n                        if __event_emitter__:\n                            await __event_emitter__(\n                                {\n                                    \"type\": \"status\",\n                                    \"data\": {\"description\": error_msg, \"done\": True},\n                                }\n                            )\n                        yield error_msg\n                        return\n\n                    collected_content = \"\"\n                    buffer = b\"\"\n\n                    async for chunk in response.content.iter_chunked(1024):\n                        buffer += chunk\n\n                        # Process complete lines\n                        while b\"\\n\" in buffer:\n                            line, buffer = buffer.split(b\"\\n\", 1)\n                            line_text = line.decode(\"utf-8\").strip()\n\n                            if not line_text:\n                                continue\n\n                            if self.valves.DEBUG:\n                                print(f\"[GEMINI] Raw line: {line_text}\")\n\n                            try:\n                                # Handle JSON lines\n                                if line_text.startswith(\"{\"):\n                                    data = json.loads(line_text)\n\n                                    if self.valves.DEBUG:\n                                        print(\n                                            f\"[GEMINI] Parsed data: {json.dumps(data, indent=2)}\"\n                                        )\n\n                                    # Extract from Gemini streaming format\n                                    if (\n                                        \"candidates\" in data\n                                        and len(data[\"candidates\"]) > 0\n                                    ):\n                                        candidate = data[\"candidates\"][0]\n                                        if (\n                                            \"content\" in candidate\n                                            and \"parts\" in candidate[\"content\"]\n                                        ):\n                                            parts = candidate[\"content\"][\"parts\"]\n                                            if len(parts) > 0 and \"text\" in parts[0]:\n                                                text_chunk = parts[0][\"text\"]\n                                                if text_chunk:\n                                                    collected_content += text_chunk\n                                                    if self.valves.DEBUG:\n                                                        print(\n                                                            f\"[GEMINI] Yielding chunk: {text_chunk}\"\n                                                        )\n                                                    yield text_chunk\n\n                            except json.JSONDecodeError as e:\n                                if self.valves.DEBUG:\n                                    print(\n                                        f\"[GEMINI] JSON decode error: {e} for line: {line_text}\"\n                                    )\n                                continue\n\n                    # Process any remaining data in buffer\n                    if buffer:\n                        try:\n                            line_text = buffer.decode(\"utf-8\").strip()\n                            if line_text and line_text.startswith(\"{\"):\n                                data = json.loads(line_text)\n                                if \"candidates\" in data and len(data[\"candidates\"]) > 0:\n                                    candidate = data[\"candidates\"][0]\n                                    if (\n                                        \"content\" in candidate\n                                        and \"parts\" in candidate[\"content\"]\n                                    ):\n                                        parts = candidate[\"content\"][\"parts\"]\n                                        if len(parts) > 0 and \"text\" in parts[0]:\n                                            text_chunk = parts[0][\"text\"]\n                                            if text_chunk:\n                                                collected_content += text_chunk\n                                                if self.valves.DEBUG:\n                                                    print(\n                                                        f\"[GEMINI] Final chunk: {text_chunk}\"\n                                                    )\n                                                yield text_chunk\n                        except Exception as e:\n                            if self.valves.DEBUG:\n                                print(f\"[GEMINI] Buffer processing error: {e}\")\n\n                    if self.valves.DEBUG:\n                        print(f\"[GEMINI] Total collected content: {collected_content}\")\n\n                    # If no content was streamed, fallback to non-streaming\n                    if not collected_content:\n                        if self.valves.DEBUG:\n                            print(\n                                \"[GEMINI] No content streamed, falling back to non-streaming\"\n                            )\n\n                        # Try non-streaming as fallback\n                        fallback_url = url  # Original generateContent URL\n                        async with session.post(\n                            fallback_url, headers=headers, json=payload, timeout=60\n                        ) as fallback_response:\n                            if fallback_response.status == 200:\n                                fallback_text = await fallback_response.text()\n                                fallback_data = json.loads(fallback_text)\n                                if (\n                                    \"candidates\" in fallback_data\n                                    and len(fallback_data[\"candidates\"]) > 0\n                                ):\n                                    candidate = fallback_data[\"candidates\"][0]\n                                    if (\n                                        \"content\" in candidate\n                                        and \"parts\" in candidate[\"content\"]\n                                    ):\n                                        parts = candidate[\"content\"][\"parts\"]\n                                        if len(parts) > 0 and \"text\" in parts[0]:\n                                            yield parts[0][\"text\"]\n\n                    if __event_emitter__:\n                        await __event_emitter__(\n                            {\n                                \"type\": \"status\",\n                                \"data\": {\n                                    \"description\": \"Stream completed\",\n                                    \"done\": True,\n                                },\n                            }\n                        )\n        except Exception as e:\n            error_msg = f\"Stream error: {str(e)}\"\n            if self.valves.DEBUG:\n                print(f\"[GEMINI] Stream exception: {error_msg}\")\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\"type\": \"status\", \"data\": {\"description\": error_msg, \"done\": True}}\n                )\n            yield error_msg\n"},"info":{"body":"Gemini API Pipe for Open WebUI **UPDATED 07/22/2025**\nA comprehensive integration pipe that connects Open WebUI with Google's Gemini API, providing access to all Gemini models including the latest Gemini 2.5 series with advanced capabilities.\nFeatures\n\nComplete Model Support: Access to all Gemini models including Gemini 2.5 Flash Preview, Gemini 2.5 Pro Preview, Gemini 2.0 Flash, and Gemini 1.5 series\nMultimodal Capabilities: Support for text and image processing across all compatible models\nStreaming Support: Real-time response streaming with automatic fallback to non-streaming mode\nVision Processing: Handle base64 encoded images and image URLs\nSystem Message Integration: Proper handling of system instructions\nDebug Mode: Comprehensive logging for troubleshooting\nError Handling: Robust error management with user-friendly feedback\n\nSupported Models\nModelContext LengthVision SupportDescriptionGemini 2.5 Flash Preview1M tokens✅Best price-performance with thinking supportGemini 2.5 Pro Preview1M tokens✅Most powerful thinking modelGemini 2.0 Flash1M tokens✅Newest multimodal modelGemini 2.0 Flash Lite1M tokens✅Optimized for cost and low latencyGemini 1.5 Flash1M tokens✅Fast and versatile performanceGemini 1.5 Flash 8B1M tokens✅High volume tasksGemini 1.5 Pro2M tokens✅Complex reasoning tasks\nInstallation\n\nDownload the Pipe: Save the provided Python code as gemini_pipe.py\nInstall in Open WebUI:\n\nNavigate to your Open WebUI admin panel\nGo to Settings → Pipelines\nClick Add Pipeline\nUpload the gemini_pipe.py file\n\n\nConfigure API Key:\n\nAfter installation, go to pipeline settings\nSet your GEMINI_API_KEY (replace \"Your API Key Here\")\n\n\n\nConfiguration\nRequired Settings\n\nGEMINI_API_KEY: Your Google AI Studio API key\n\nGet your API key from Google AI Studio\nReplace the default \"Your API Key Here\" value\n\n\n\nOptional Settings\n\nDEBUG: Enable/disable debug logging (default: True)\nFORCE_NON_STREAMING: Force non-streaming mode (default: False)\n\nGetting Your API Key\n\nVisit Google AI Studio\nSign in with your Google account\nClick \"Create API Key\"\nCopy the generated key\nPaste it into the GEMINI_API_KEY field in the pipeline settings\n\nUsage\nBasic Text Chat\nOnce installed and configured, simply:\n\nSelect a Gemini model from the model dropdown\nStart chatting normally\nThe pipe handles all API communication transparently\n\nMultimodal (Text + Images)\nThe pipe automatically handles:\n\nBase64 Images: Directly embedded in chat\nImage URLs: Converted to appropriate format\nMixed Content: Text and images in the same message\n\nSystem Instructions\nSystem messages are automatically converted to Gemini's format and prepended to conversations.\nFeatures in Detail\nStreaming Support\n\nReal-time Responses: Get responses as they're generated\nAutomatic Fallback: If streaming fails, automatically switches to non-streaming\nConfigurable: Can be disabled via FORCE_NON_STREAMING setting\n\nError Handling\n\nAPI Key Validation: Checks for missing or default API keys\nHTTP Error Handling: Proper error messages for API failures\nGraceful Degradation: Fallback mechanisms for various failure scenarios\n\nDebug Mode\nWhen enabled, provides detailed logging:\n\nRequest/response data\nModel name transformations\nStreaming vs non-streaming decisions\nError details and stack traces\n\nTroubleshooting\nCommon Issues\n\n\"GEMINI_API_KEY is required\" Error\n\nEnsure you've set your API key in the pipeline settings\nVerify the key is not the default \"Your API Key Here\"\n\n\nHTTP 400 Errors\n\nCheck that your API key is valid and active\nVerify the model name is supported\n\n\nHTTP 429 Errors\n\nYou've exceeded API rate limits\nWait and retry, or upgrade your Google AI Studio quota\n\n\nStreaming Issues\n\nEnable debug mode to see detailed logs\nTry setting FORCE_NON_STREAMING to True\n\n\n\nDebug Mode\nEnable debug mode in settings to see:\n[GEMINI] Original model: gemini-2.5-flash-preview-04-17\n[GEMINI] Clean model: gemini-2.5-flash-preview-04-17\n[GEMINI] URL: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent?key=***\n[GEMINI] Stream requested: True\n[GEMINI] Using streaming: True\nAPI Endpoints Used\n\nNon-streaming: https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent\nStreaming: https://generativelanguage.googleapis.com/v1beta/models/{model}:streamGenerateContent\n\nRequirements\n\nOpen WebUI: Compatible with Open WebUI pipeline system\nPython Dependencies:\n\naiohttp: For async HTTP requests\npydantic: For configuration validation\njson: For data serialization\nlogging: For debug output\n\n\n\nContributing\nThis pipe is designed to be:\n\nExtensible: Easy to add new models or features\nMaintainable: Clear code structure and documentation\nRobust: Comprehensive error handling and fallbacks\n\nLicense\nThis code is provided as-is for integration with Open WebUI and Google's Gemini API. Please ensure compliance with Google's API terms of service and Open WebUI's licensing requirements.\nSupport\nFor issues:\n\nEnable debug mode and check logs\nVerify API key and model availability\nCheck Google AI Studio status and quotas\nConsult Open WebUI documentation for pipeline issues\n\ninstagram @p0u\n\nNote: This pipe requires an active Google AI Studio API key. Some models may have usage restrictions or require special access."},"downloads":203,"upvotes":0,"downvotes":0,"updatedAt":1753201398,"createdAt":1752279019,"user":{"id":"d9330366-68f4-451f-a001-10bf00b11fd3","username":"p0us","name":"zarpil","createdAt":1752278772,"role":null,"verified":false}}]